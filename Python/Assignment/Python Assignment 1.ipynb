{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMPBA 2021 Winter - Python Assignment\n",
    "\n",
    "## Deep Kamal Singh - deepkamal_singh_2021w@isb.edu\n",
    "## Student ID: 000012020053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20-Oct-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Suppose the cover price of a book is Rs. 200, but bookstores get a 25% discount. Shipping costs Rs. 40 for the first copy and Rs. 10 for each additional copy. Write a Python program to calculate the total wholesale cost for 60 copies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Answer to Q1\n",
    "coverPrice=200\n",
    "storeDiscount=1/4\n",
    "shippingCostFirstCopy=40\n",
    "shippingCostSubsequentCopy=10\n",
    "copies=60\n",
    "totalCost=copies*(coverPrice-coverPrice*storeDiscount)+shippingCostFirstCopy+shippingCostSubsequentCopy*(copies-1)\n",
    "totalCost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. A number,x,is a power of y if it is divisible by y and x/y is a power of y. Write a function in Python that takes parameters x and y and returns True if x is a power of y.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iCheckPower(x, y):\n",
    "    #if x=y then its x^1=y , we are checking if remainder is 0 i.e y fully divides x, then recursively checking \n",
    "    return True if x == y or (((x % y) == 0) and iCheckPower((x / y), y)) else False\n",
    "\n",
    "iCheckPower(65536,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. Write a function in Python that takes three parameters a, b, and c and returns their greatest common divisor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcdFinder(a,b,c):\n",
    "    def factors(n):\n",
    "        # set () <= so we dont get repeating numbers\n",
    "        # n**0.5 means taking square root of n, we are iterating i from 1 to sqrt of n, as all factors lie within Sqrt of a number \n",
    "        # we are iterating `factor` from 1 to n//i, \n",
    "        # Double slash operator // is used to get floor after division, so result is Integer\n",
    "        # n%i is where we are checking if i'th iteration leaves 0 as remainder\n",
    "        return set( factor for i in range(1, int(n**0.5) + 1) if n % i == 0 for factor in (i, n//i) )\n",
    "    \n",
    "    factorA = factors(a)\n",
    "    factorB = factors(b)\n",
    "    factorC = factors(c)\n",
    "    #set intersection will result in common factors among a,b,c and Max will give us greatest common divisor/factor\n",
    "    return max(factorA.intersection(factorB).intersection(factorC))\n",
    "\n",
    "gcdFinder(106,53,106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Write a Python script that reads the current time and converts it to a time of day in hours, minutes, and seconds, plus the number of days since the epoch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t=time.time() #time.time() gives number of seconds since EPOC i.e 1 Jan 1970 00:00:00\n",
    "day= t//86400 #1 day has 86400 seconds, `day` will store number of days since EPOCH\n",
    "tt=t%86400    # remaining seconds of the day will be used to get Hour, minute and seconds\n",
    "\n",
    "print('Days since EPOCH is',int(day),', Time(in UTC) is', int(tt//3600),\":\",int((tt%3600)//60),\":\", int(((tt%3600)%60)//60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 5.Write a function in Python that takes three integers as arguments, and that prints either “Yes” or “No”, depending on whether you can or cannot form a triangle from the given lengths.**\n",
    "    \n",
    "    Hint: If any of the three lengths is greater than the sum of the other two, then you cannot form a triangle. Otherwise, you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkTriangle(a,b,c):\n",
    "    #If any of the three lengths is greater than the sum of the other two, then its not a triangle.\n",
    "    return  (c<(a+b)) and  (a<(b+c)) and (b<(a+c))\n",
    "\n",
    "checkTriangle(32,19,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. The following functions are all intended to check whether a string contains any uppercase letters, but at least some of them are wrong. For each function, describe what the function actually does, assuming that the parameter is a string.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a)\n",
    "#This function will iterate over all characters in provided string s\n",
    "# further it checks if chars are in upper case using isupper() method\n",
    "# when a uppercase char is encountered, it will return true hence loop will breakout and function will end.\n",
    "def any_uppercase1(s): \n",
    "    for c in s:\n",
    "        if c.isupper(): \n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "#(b)\n",
    "# This function will iterate over all characters in provided string s.\n",
    "# However it will check if 'c' character constant is in uppercase with each iteration, instead of checking iteration variable c \n",
    "# further it will return String instead of boolean, \n",
    "# important to note that it doesnt matter what values are passed in this function, it'll always return 'False'\n",
    "\n",
    "def any_uppercase2(s): \n",
    "    for c in s:\n",
    "        if 'c'.isupper(): \n",
    "            return 'True'\n",
    "        else:\n",
    "            return 'False'\n",
    "#(c)\n",
    "# This function will iterate over all characters in provided string s.\n",
    "# it will change value of `flag` to true or false depending upon current iteration character being in upper or lower\n",
    "# however it keeps running the loop, thus it'll return True only if last character of provided string is in uppercase\n",
    "# in all other case it will return False \n",
    "def any_uppercase3(s): \n",
    "    for c in s:\n",
    "        flag = c.isupper() \n",
    "    return flag\n",
    "#(d)\n",
    "\n",
    "# This function will iterate over all characters in provided string s.\n",
    "# it will change value of `flag` to True when it meets first uppercase letter in s\n",
    "# there on it will always be True as `or` operation is applied,\n",
    "# thus Function will return True if even single letter in the string is in uppercase, \n",
    "# this function does exactly what any_uppercase1 is doing , just with extra iterations.\n",
    "def any_uppercase4(s): \n",
    "    flag = False\n",
    "    for c in s:\n",
    "        flag = flag or c.isupper()\n",
    "    return flag\n",
    "#(e)\n",
    "# This function will iterate over all characters in provided string s.\n",
    "# it will return False at first character it finds in lowercase\n",
    "# if all chars are in uppercase then only it will return True\n",
    "\n",
    "def any_uppercase5(s): \n",
    "    for c in s:\n",
    "        if not c.isupper(): return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. Write a function in Python that takes a list of numbers and returns the cumulative sum; that is, a new list where the nth element is the sum of the first n + 1 elements from the original list.**\n",
    "\n",
    "Example:\n",
    "\n",
    "    >>> t = [1, 2, 3]\n",
    "    \n",
    "    >>> cumulative_sum(t) [1, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumSum(l):\n",
    "    ret=list();\n",
    "    val=0;\n",
    "    for s in l:\n",
    "        ret.append(s+val)\n",
    "        val+=s\n",
    "    return ret\n",
    "    \n",
    "cumSum([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. Write a Python program that reads a file, breaks each line into words, strips whitespace and punctuation from the words, and converts them to uppercase.**\n",
    "\n",
    "    Hint : The string module provides a string named whitespace, which contains space, tab, new--‐line, etc., and punctuation which contains the punctuation characters.\n",
    "    >>> import string\n",
    "    >>> string.punctuation\n",
    "    '!\"#$%&'()*+,-­‐./:;<=>?@[\\]^_`{|}~'\n",
    "    \n",
    "    Also, you might consider using the string methods strip, replace and translate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONTENT OF THE FILE ###\n",
    "'''\n",
    "Post management tools menu\n",
    "\n",
    "\n",
    "Upgrade\n",
    "Deep Kamal Singh\n",
    "Adobe Tech Blog\n",
    "ADOBE EXPERIENCE PLATFORM & ADOBE I/O\n",
    "\n",
    "Magento 2.4 Enhanced security with Two-Factor authentication\n",
    "In this article\n",
    "Short theory about Magento’s Two-Factor Authentication.\n",
    "A quick view of CLI Command to install Magento 2.4.0 for tl;dr; folks.\n",
    "Setup Google Authenticator for Magento TFA (aka 2FA).\n",
    "Create Magento Admin user with Google Authenticator selected for TFA.\n",
    "Install Google Authenticator app (iOS | Android ) on Mobile and configure it for your local Magento access.\n",
    "Login to Admin panel with TFA cleared like a boss (even if you don’t have email configured).\n",
    "Two-factor authentication is now a well-proven standard for added security in the login process, and it makes perfect sense that Magento 2.4.0 has two-factor authentication enabled on admin panel logins by default. Configuring it is easier than you’d have thought.\n",
    "We can install any two-factor authenticator provider in Magento using the standard composer require and module enable process, by default Magento Commerce 2.4 ships with these providers:\n",
    "$ bin/magento security:tfa:providers\n",
    "google: Google Authenticator\n",
    "duo_security: Duo Security\n",
    "authy: Authy\n",
    "u2fkey: U2F (Yubikey and others)\n",
    "And it’s up to the provider to decide how OTP will be generated and verifie`d`, 'once a provider is configured and linked with Magento, the OTP is generated by the provider’s tool which is disconnected from the Magento system, the OTP is then entered manually on the Magento login screen by Admin user - Magento hands over the entered digits to Provider’s Magento module for verification. OTP can be received via SMS, Email, SNS/SQS, FCM — and the most direct way is to 'use a generator/verifier app which makes the entire auth process independent of the aforementioned carrier technologies.\n",
    "In this article, we will learn how to configure and enable TFA on Magento 2.4 exactly on [this] principle — you just need an Android or iOS device.\n",
    "[Basic installation\n",
    "]If you are trying your hands on freshly brewed hot and ready to serve Magento 2.4.0, then the first thing you’d notice is there’s no Web Setup wizard, like in previous versions. You have to do the installation over CLI. So, let me help you right away with a TL;DR ‘ready to fire’ install command (I assume you already have Magento 2.4.0 downloaded and trust you will be able to change values in arguments below as per your env)\n",
    "magento setup:install --base-url=\"https://local.magento/\" --db-host=localhost --db-name=\"magento\" --db-user=\"dbuser\" --db-password=\"dbpassword\" --admin-firstname=\"Magento\" --admin-lastname=\"Admin\" --admin-email=\"magento@adobe.com\" --admin-user=\"admin\" --admin-password=\"xxyyzzaabbcc\" --language=\"en_US\" --currency=\"USD\" --timezone=\"Asia/Kolkata\" --use-rewrites=0 --backend-frontname=\"admin\" --cleanup-database\n",
    "Now that we have a Magento v2.4 installed and we can access the store-front and backend, we are going to login to Magento backend. This will ask for user and password, no big deal — but after we login successfully, we don’t see the dashboard this time, we will see a new Magento screen now:\n",
    "Image for post\n",
    "Bummer! right? well No actually, it’s easy to get the two-factor authentication working, read on\n",
    "{If you have a mail server configured}you will receive a{n email} from your newly installed Magento 2.4 a link to configure your authenticator first. The first time you get an option to select between Google or Authy or any other enabled provider, for ease of use, just choose Google Authenticator. You will get a QR code image generated by Magento admin on the same screen just scan the QR through Google Authenticator app (iOS |Android ) and voila! you are good to go.\n",
    "BUT what to do if you don’t have mail server like postfix or something similar configured on your Magento system, then there is no clear way to get the authenticator link (unless you have postfix or similar server running and you know how to extract mail logs and read the content of unsent stuck mail, then you might need to Base64 decode it and do some string editing to get the URL link),\n",
    "No mail server? No problem!\n",
    "At the Magento{ Two-Factor configuration screen, just take a deep }breath and mumble “I’ll be back,” and calmly log out.\n",
    "And then here is what you can do —\n",
    "Set Google as Two-factor auth provider in Database:\n",
    "Login to MySQL console and first check if there is a security provider already set in core configs:\n",
    "select * from core_config_data where path like '%provider%';\n",
    "If the above response is Empty set (0.03 sec) , then go ahead and insert a config that conforms Magento to use Google as TFA provider:\n",
    "insert into core_config_data(scope,scope_id,path,`value`) values ('default',0,'twofactorauth/general/force_providers','google');\n",
    "'''\n",
    "import string\n",
    "filePath = '/Users/dks/Desktop/anHTML.html'\n",
    "  \n",
    "def punctuationReplacer(aLine):\n",
    "    pns = string.punctuation\n",
    "    for x in pns:aLine=aLine.replace(x,'')\n",
    "    return aLine\n",
    "        \n",
    "#RESULT\n",
    "res=[punctuationReplacer(aLine.strip().upper()) for aLine in open(filePath,'r').read().split('\\n')]\n",
    "\n",
    "#we are applying filter to remove blank entries that would otherwise return to blank lines\n",
    "print('\\n'.join(filter(None,res)))\n",
    "#we notice that char ’ is not getting replaced, its differnt from ' char - \n",
    "# this is because string.punctuation does not have ’ character "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9. Write a program that searches a directory and all of its subdirectories recursively, and returns a list of complete paths for all files with a given suffix (like .mp3).**\n",
    "\n",
    "    Hint: os.path provides several useful functions for manipulating file and path names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osPath\n",
    "\n",
    "def beginSearch(rootPath, suffix):\n",
    "    retList = []\n",
    "    try:\n",
    "        dirContent = os.listdir(rootPath)\n",
    "        for aContent in dirContent:\n",
    "            aPath = rootPath + \"/\" + aContent\n",
    "            if osPath.isdir(aPath):\n",
    "                retList += beginSearch(aPath,suffix)\n",
    "            else:\n",
    "                if aPath.endswith(suffix):\n",
    "                    retList.append(aPath)\n",
    "        return retList\n",
    "    except:\n",
    "        return retList\n",
    "print([a for a in beginSearch('/Users/Deesingh/Documents', 'pdf')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10. Analyze Earthquakes**\n",
    "\n",
    "The National Earthquake Information Center (NEIC) determines the location and size of all significant earthquakes that occur worldwide and disseminates this information immediately to national and international agencies, scientists, critical facilities, and the general public. The NEIC compiles and provides to scientists and to the public an extensive seismic database that serves as a foundation for scientific research through the operation of modern digital national and global seismograph networks and cooperative international agreements. The NEIC is the national data center and archive for earthquake information.\n",
    "This dataset includes a record of the date, time, location, depth, magnitude, and source of every earthquake with a reported magnitude 5.5 or higher since 1965.\n",
    "Conduct data analysis on the dataset and try to uncover meaningful and/or interesting insights from the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Q10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import relevant libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading dataset, and listing columns (variables)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser as dateParser\n",
    "\n",
    "eqDF=pd.read_csv('/Users/dks/Desktop/AMPBA/Python/Assignment/earthquake.csv')\n",
    "\n",
    "#checking columns(variables) on Data Set\n",
    "print(eqDF.shape);\n",
    "eqDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Lets see 5 point Summary of each numeric column***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets study the data by doing a describe on entire DF, \n",
    "#describe() operation spreads and returns column stats of each col with numerical type\n",
    "#we are summarizing the data using describe, this will provide univariate analysis of each data points\n",
    "eqDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insights from 5 point summary***\n",
    "\n",
    " - **Depth**: Max depth of recorded earthquake is 700 units. There is minimum value `-1.10` - which indicates something happened above ground level that caused the earthquake. On an average earthquakes are originated at 70.77 units below ground level, however 75% are with 54 units - this indicates there are outliers in data impacting the mean value. Similar analysis can be drawn with `Depth Seismic Stations` series\n",
    " \n",
    " - **Magnitude** - mean magnitude is 5.88 units, and std deviation is less than 0.5, further inter-quartile variations do not change higher than std dev, meaning the magnitude data is losely uniform - however there are outliers such as max value which is approximately `[mean + 40X(std dev)]`\n",
    " \n",
    " - **Magnitude Error** - errors are available in 327 observations out of 23412, indicates that probability of errors in magnitude is low, however not negligible, it can be correlated with Sources column to derive at error margin of a perticular source.\n",
    " \n",
    " - Though subject matter expertise is required to understand several columns (like `Azimuthal Gap`, `Horizontal Distance`, `Root Mean Square` etc) we can draw inferences by finding covariance and correlation between available values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Lets analyse covariance table of Data Set***\n",
    "\n",
    "A covariance table is Covariance listingg of each variable with respect to other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqDF.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Lets analyse Correlation table also***\n",
    "\n",
    "Correlation is Covariance divided by product of std Deviations, this ranges from -1 to +1 , correlation table is basically NxN matrix of correlations between each numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eqDF.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations with moderate to high (> 0.2 or < -0.2) Correlation Coefficient (Pearson's)***\n",
    "\n",
    "- `Latitude` with `Depth Seismic Stations` : 0.433 - shows moderate positive correlation\n",
    "- `Latitude` with `Magnitude Sesmic Stations` : 0.315 - Shows moderate positive correlation\n",
    "- `Latitude` with `Horizontal Distance`: -0.397 - Shows moderate negative correlation\n",
    "- `Longitude` is not having a high +ve or -ve correlation with rest of data points\n",
    "- `Depth` is also not having a high +ve or -ve correlation with rest of data points\n",
    "- `Depth Error` is showing moderate correlation with `Depth Seismic Stations` of -0.320 ,and its showing High correlation with `Magnitude Error` of 0.618 and with `Horizontal Error` of 0.644 - we can infer that Error occurences are correlated with each other.\n",
    "- `Depth Seismic Stations` is as noted earlier is correlated to `Latitude` with coeff. of `0.433`. It is also correlated to `Magnitude` with `0.440` coefficient value, this indicates that seismic stations are  near areas with high probability of Earthquakes, \n",
    "- `Depth Seismic Station`'s correlation coeff with `Depth Error`, `Magnitude Error` and `Horizontal Error` are all -ve\n",
    "- `Magnitude Seismic Stations` is also correlated with `Latitude` coeff. value of 0.315 \n",
    "\n",
    "likewise many inferences can be drawn with respect to correlation, however Subject Matter expertise will be required further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now Lets plot `Magnitude` and other relevant stats over `Year`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting year and assigning to new column - noticed that in 3 observations Date is formatted in different manner, lambda function will handle the format\n",
    "eqDF['eventYear']=eqDF['Date'].apply(lambda dt:int(dt.split('/')[2] if len(dt)==10 else dt.split('-')[0])).sort_values(ascending=False)\n",
    "eqDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqDF[['eventYear','Magnitude']].plot(x='eventYear',alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the YoY Earth quake occurances we can see there is a approximately cyclic pattern - but maginitudes are marginally increasing also YoY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can further analyze by grouping by year and plotting over count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqDF[['eventYear','Magnitude']].groupby(['eventYear']).count().plot(title=\"Magnitude over Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that magnitude of earthquakes are increasing over period of time OR data recording is becoming more accurate YoY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let plot earthquake occurance over Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqDF[['Latitude','Magnitude']].groupby(['Latitude']).count().plot(title=\"Magnitude over Latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqDF[['Longitude','Magnitude']].groupby(['Longitude']).count().plot(title=\"Magnitude over Longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eqDF.plot(figsize=(15,4))\n",
    "# eqDF.plot(subplots=True, figsize=(15,6))\n",
    "# eqDF.plot(y=[\"Depth\", \"Magnitude\"], figsize=(15,4))\n",
    "eqDF.plot(x=\"Date\", y=[\"Magnitude Error\"])\n",
    "# eqDF[['Longitude','Latitude']].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above plot, it shows concentrated Magnitude errors after 2010 in time series, \n",
    "this may be the indicator that error margin was not recorded or some new mechanism was adopted to calculate errors in readings, alternatively (less likely) it may indicate some malfunction after 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets summarize further by date of occurance of events Grouping by date to get the count of occurance on each date \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are grouping by Type and summarizing the data - to get a gist of types of earthquakes included in the dataset\n",
    "# this gives us information that there are primarily 4 types of events that cause earthquakes - naturally occuring earthquakes, Nuclear explosion, other explosion, and Rock burst\n",
    "# there are 175 earthquakes caused by Nuclear Explosions - human caused\n",
    "print(eqDF[['Type','Magnitude']].groupby(['Type']).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plotting Earthquake occurances over world map***\n",
    "\n",
    "Now we will use Plotly library to plot occurance over map and see if we can draw some insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scatter_geo(data_frame, lat, lon, locations, locationmode, color, text, \n",
    "#     hover_name, hover_data, custom_data, size, animation_frame, animation_group, \n",
    "#     category_orders, labels, color_discrete_sequence, color_discrete_map, color_continuous_scale, \n",
    "#     range_color, color_continuous_midpoint, opacity, size_max, projection, scope, center, title, \n",
    "#     template, width, height)\n",
    "fig = px.scatter_geo(data_frame=eqDF,color=\"Type\",\n",
    "                    lat=eqDF.Latitude,\n",
    "                    lon=eqDF.Longitude,\n",
    "                    hover_name=\"Magnitude\",\n",
    "                    title=\"Plotting Earthquakes,Nuclear explosions and others on geomap\",\n",
    "                    projection=\"natural earth\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this gives us clear insight of Earthquake prone zones across the globe, a finer analysis can be made with these specific Latitude and Longitude loci to drawyearly and monthly trends and periodic patterns can be found if exists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.The dataset contains data on Indian cities (sourced from Govt of India website) and has information about various characteristics.**\n",
    "\n",
    "    Columns for the dataset are as follows:\n",
    "\n",
    "    'name_of_city'\t: Name of the City\n",
    "    'state_code'\t: State Code of the City \n",
    "    'state_name'\t: State Name of the City\n",
    "    'dist_code'\t: District Code where the city belongs ( 99 means multiple District)\n",
    "    'population_total'\t: Total Population \n",
    "    'population_male'\t: Male Population\n",
    "    'population_female'\t: Female Population\n",
    "    '0--‐6_population_total'\t: 0--‐6 Age Total Population \n",
    "    '0--‐6_population_male'\t: 0--‐6 Age Male Population\n",
    "    '0--‐6_population_female\t: 0--‐6 Age Female Population \n",
    "    'literates_total'\t: Total Literates \n",
    "    'literates_male'\t: Male Literates\n",
    "    'literates_female'\t: Female Literates\n",
    "    'sex_ratio'\t: Sex Ratio\n",
    "    'child_sex_ratio'\t: Sex Ratio 0-6\n",
    "    'effective_literacy_rate_total' : Literacy rate over Age 7\n",
    "    'effective_literacy_rate_male'  : Male Literacy rate over Age 7\n",
    "    'effective_literacy_rate_female': Female Literacy rate over Age 7\n",
    "    'location'  : Lat,Lng\n",
    "    'total_graduates'   : Total Number of Graduates\n",
    "    'male_graduates'    : Male Graduates\n",
    "    'female_graduates'  : Female Graduates\n",
    "\n",
    "**Conduct data analysis on the dataset and try to uncover meaningful and/or interesting insights from the dataset. Before conducting the analyses state the questions that you would want to address from the dataset, and then conduct analyses accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name_of_city', 'state_code', 'state_name', 'dist_code',\n",
      "       'population_total', 'population_male', 'population_female',\n",
      "       '0-6_population_total', '0-6_population_male', '0-6_population_female',\n",
      "       'literates_total', 'literates_male', 'literates_female', 'sex_ratio',\n",
      "       'child_sex_ratio', 'effective_literacy_rate_total',\n",
      "       'effective_literacy_rate_male', 'effective_literacy_rate_female',\n",
      "       'location', 'total_graduates', 'male_graduates', 'female_graduates'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>dist_code</th>\n",
       "      <th>population_total</th>\n",
       "      <th>population_male</th>\n",
       "      <th>population_female</th>\n",
       "      <th>0-6_population_total</th>\n",
       "      <th>0-6_population_male</th>\n",
       "      <th>0-6_population_female</th>\n",
       "      <th>literates_total</th>\n",
       "      <th>literates_male</th>\n",
       "      <th>literates_female</th>\n",
       "      <th>sex_ratio</th>\n",
       "      <th>child_sex_ratio</th>\n",
       "      <th>effective_literacy_rate_total</th>\n",
       "      <th>effective_literacy_rate_male</th>\n",
       "      <th>effective_literacy_rate_female</th>\n",
       "      <th>total_graduates</th>\n",
       "      <th>male_graduates</th>\n",
       "      <th>female_graduates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>493.000000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>4.930000e+02</td>\n",
       "      <td>4.930000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.643002</td>\n",
       "      <td>16.782961</td>\n",
       "      <td>4.481124e+05</td>\n",
       "      <td>2.343468e+05</td>\n",
       "      <td>2.137656e+05</td>\n",
       "      <td>4.709285e+04</td>\n",
       "      <td>24849.527383</td>\n",
       "      <td>22243.320487</td>\n",
       "      <td>3.461527e+05</td>\n",
       "      <td>1.894384e+05</td>\n",
       "      <td>1.567143e+05</td>\n",
       "      <td>930.294118</td>\n",
       "      <td>902.332657</td>\n",
       "      <td>85.131460</td>\n",
       "      <td>89.920162</td>\n",
       "      <td>79.967181</td>\n",
       "      <td>6.620236e+04</td>\n",
       "      <td>3.771556e+04</td>\n",
       "      <td>2.848680e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.297168</td>\n",
       "      <td>15.566131</td>\n",
       "      <td>1.033228e+06</td>\n",
       "      <td>5.487786e+05</td>\n",
       "      <td>4.848622e+05</td>\n",
       "      <td>1.050279e+05</td>\n",
       "      <td>55535.310272</td>\n",
       "      <td>49523.241379</td>\n",
       "      <td>8.220952e+05</td>\n",
       "      <td>4.534753e+05</td>\n",
       "      <td>3.690677e+05</td>\n",
       "      <td>55.849106</td>\n",
       "      <td>49.794689</td>\n",
       "      <td>6.186345</td>\n",
       "      <td>5.377492</td>\n",
       "      <td>7.577825</td>\n",
       "      <td>1.778187e+05</td>\n",
       "      <td>9.849574e+04</td>\n",
       "      <td>7.951556e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000360e+05</td>\n",
       "      <td>5.020100e+04</td>\n",
       "      <td>4.512600e+04</td>\n",
       "      <td>6.547000e+03</td>\n",
       "      <td>3406.000000</td>\n",
       "      <td>3107.000000</td>\n",
       "      <td>5.699800e+04</td>\n",
       "      <td>3.475100e+04</td>\n",
       "      <td>2.224700e+04</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>49.510000</td>\n",
       "      <td>52.270000</td>\n",
       "      <td>46.450000</td>\n",
       "      <td>2.532000e+03</td>\n",
       "      <td>1.703000e+03</td>\n",
       "      <td>8.290000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.261420e+05</td>\n",
       "      <td>6.638400e+04</td>\n",
       "      <td>6.041100e+04</td>\n",
       "      <td>1.363900e+04</td>\n",
       "      <td>7221.000000</td>\n",
       "      <td>6457.000000</td>\n",
       "      <td>9.768700e+04</td>\n",
       "      <td>5.357800e+04</td>\n",
       "      <td>4.391400e+04</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>868.000000</td>\n",
       "      <td>81.750000</td>\n",
       "      <td>87.280000</td>\n",
       "      <td>75.800000</td>\n",
       "      <td>1.527700e+04</td>\n",
       "      <td>9.289000e+03</td>\n",
       "      <td>6.114000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.841330e+05</td>\n",
       "      <td>9.665500e+04</td>\n",
       "      <td>8.776800e+04</td>\n",
       "      <td>1.944000e+04</td>\n",
       "      <td>10342.000000</td>\n",
       "      <td>9172.000000</td>\n",
       "      <td>1.413290e+05</td>\n",
       "      <td>7.590600e+04</td>\n",
       "      <td>6.383600e+04</td>\n",
       "      <td>922.000000</td>\n",
       "      <td>903.000000</td>\n",
       "      <td>85.970000</td>\n",
       "      <td>91.180000</td>\n",
       "      <td>80.920000</td>\n",
       "      <td>2.395900e+04</td>\n",
       "      <td>1.404900e+04</td>\n",
       "      <td>9.558000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.490330e+05</td>\n",
       "      <td>1.750550e+05</td>\n",
       "      <td>1.700260e+05</td>\n",
       "      <td>3.794500e+04</td>\n",
       "      <td>19982.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>2.679000e+05</td>\n",
       "      <td>1.455480e+05</td>\n",
       "      <td>1.235030e+05</td>\n",
       "      <td>971.000000</td>\n",
       "      <td>942.000000</td>\n",
       "      <td>89.330000</td>\n",
       "      <td>93.400000</td>\n",
       "      <td>85.400000</td>\n",
       "      <td>5.036700e+04</td>\n",
       "      <td>2.787200e+04</td>\n",
       "      <td>2.086600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.247845e+07</td>\n",
       "      <td>6.736815e+06</td>\n",
       "      <td>5.741632e+06</td>\n",
       "      <td>1.209275e+06</td>\n",
       "      <td>647938.000000</td>\n",
       "      <td>561337.000000</td>\n",
       "      <td>1.023759e+07</td>\n",
       "      <td>5.727774e+06</td>\n",
       "      <td>4.509812e+06</td>\n",
       "      <td>1093.000000</td>\n",
       "      <td>1185.000000</td>\n",
       "      <td>98.800000</td>\n",
       "      <td>99.300000</td>\n",
       "      <td>98.310000</td>\n",
       "      <td>2.221137e+06</td>\n",
       "      <td>1.210040e+06</td>\n",
       "      <td>1.011097e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_code   dist_code  population_total  population_male  \\\n",
       "count  493.000000  493.000000      4.930000e+02     4.930000e+02   \n",
       "mean    18.643002   16.782961      4.481124e+05     2.343468e+05   \n",
       "std      9.297168   15.566131      1.033228e+06     5.487786e+05   \n",
       "min      1.000000    1.000000      1.000360e+05     5.020100e+04   \n",
       "25%      9.000000    7.000000      1.261420e+05     6.638400e+04   \n",
       "50%     19.000000   13.000000      1.841330e+05     9.665500e+04   \n",
       "75%     27.000000   21.000000      3.490330e+05     1.750550e+05   \n",
       "max     35.000000   99.000000      1.247845e+07     6.736815e+06   \n",
       "\n",
       "       population_female  0-6_population_total  0-6_population_male  \\\n",
       "count       4.930000e+02          4.930000e+02           493.000000   \n",
       "mean        2.137656e+05          4.709285e+04         24849.527383   \n",
       "std         4.848622e+05          1.050279e+05         55535.310272   \n",
       "min         4.512600e+04          6.547000e+03          3406.000000   \n",
       "25%         6.041100e+04          1.363900e+04          7221.000000   \n",
       "50%         8.776800e+04          1.944000e+04         10342.000000   \n",
       "75%         1.700260e+05          3.794500e+04         19982.000000   \n",
       "max         5.741632e+06          1.209275e+06        647938.000000   \n",
       "\n",
       "       0-6_population_female  literates_total  literates_male  \\\n",
       "count             493.000000     4.930000e+02    4.930000e+02   \n",
       "mean            22243.320487     3.461527e+05    1.894384e+05   \n",
       "std             49523.241379     8.220952e+05    4.534753e+05   \n",
       "min              3107.000000     5.699800e+04    3.475100e+04   \n",
       "25%              6457.000000     9.768700e+04    5.357800e+04   \n",
       "50%              9172.000000     1.413290e+05    7.590600e+04   \n",
       "75%             17954.000000     2.679000e+05    1.455480e+05   \n",
       "max            561337.000000     1.023759e+07    5.727774e+06   \n",
       "\n",
       "       literates_female    sex_ratio  child_sex_ratio  \\\n",
       "count      4.930000e+02   493.000000       493.000000   \n",
       "mean       1.567143e+05   930.294118       902.332657   \n",
       "std        3.690677e+05    55.849106        49.794689   \n",
       "min        2.224700e+04   700.000000       762.000000   \n",
       "25%        4.391400e+04   890.000000       868.000000   \n",
       "50%        6.383600e+04   922.000000       903.000000   \n",
       "75%        1.235030e+05   971.000000       942.000000   \n",
       "max        4.509812e+06  1093.000000      1185.000000   \n",
       "\n",
       "       effective_literacy_rate_total  effective_literacy_rate_male  \\\n",
       "count                     493.000000                    493.000000   \n",
       "mean                       85.131460                     89.920162   \n",
       "std                         6.186345                      5.377492   \n",
       "min                        49.510000                     52.270000   \n",
       "25%                        81.750000                     87.280000   \n",
       "50%                        85.970000                     91.180000   \n",
       "75%                        89.330000                     93.400000   \n",
       "max                        98.800000                     99.300000   \n",
       "\n",
       "       effective_literacy_rate_female  total_graduates  male_graduates  \\\n",
       "count                      493.000000     4.930000e+02    4.930000e+02   \n",
       "mean                        79.967181     6.620236e+04    3.771556e+04   \n",
       "std                          7.577825     1.778187e+05    9.849574e+04   \n",
       "min                         46.450000     2.532000e+03    1.703000e+03   \n",
       "25%                         75.800000     1.527700e+04    9.289000e+03   \n",
       "50%                         80.920000     2.395900e+04    1.404900e+04   \n",
       "75%                         85.400000     5.036700e+04    2.787200e+04   \n",
       "max                         98.310000     2.221137e+06    1.210040e+06   \n",
       "\n",
       "       female_graduates  \n",
       "count      4.930000e+02  \n",
       "mean       2.848680e+04  \n",
       "std        7.951556e+04  \n",
       "min        8.290000e+02  \n",
       "25%        6.114000e+03  \n",
       "50%        9.558000e+03  \n",
       "75%        2.086600e+04  \n",
       "max        1.011097e+06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "citiesDF=pd.read_csv('/Users/dks/Desktop/AMPBA/Python/Assignment/Indian_cities.csv')\n",
    "print(citiesDF.columns)\n",
    "citiesDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_name                    \n",
       "ANDAMAN & NICOBAR ISLANDS  376     81997.76\n",
       "ANDHRA PRADESH             2       84035.51\n",
       "                           4      101360.38\n",
       "                           29     195533.88\n",
       "                           86     108618.41\n",
       "                                    ...    \n",
       "WEST BENGAL                436    394451.46\n",
       "                           443    356184.82\n",
       "                           468     87437.18\n",
       "                           476    159907.13\n",
       "                           478    137960.16\n",
       "Length: 493, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citiesDF[['state_name','literates_total','effective_literacy_rate_total']].groupby('state_name').apply(sum,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
