{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b91c2c4-09ff-4b2c-8adc-152815336ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e0b866-5d8c-4390-9de9-fd269aaaf5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fb68f5-c91d-42b5-9a0d-5409c832c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SparkFiles\n",
    "from pyspark.sql import *\n",
    "\n",
    "\n",
    "config_prefix = \"ampba.batch15.bdm2.group_assignment\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"BDM2-GroupAssignment\") \\\n",
    ".config(config_prefix+\".raw_covid_datasource\", \"https://api.covid19india.org/csv/latest/raw_data{file_index}.csv\") \\\n",
    ".config(config_prefix+\".raw_covid_datasource_file_upto\", 27) \\\n",
    ".config(config_prefix+\".covid_datasource\", \"https://api.covid19india.org/csv/latest/districts.csv\") \\\n",
    ".config(config_prefix+\".stock_datasource\", \"https://docs.google.com/spreadsheets/d/1sNXNbIrOSU6jdJwFHOY6FGGM8PsJrNdkWf_70WOPtjc/export?format=csv\") \\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9390da1b-884f-4d9f-b220-cd37ff8fd223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BDM2-GroupAssignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112666700>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3713586a-db5b-4f59-860a-4f55b2a3bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching first raw data file\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data1.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data2.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data3.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data4.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data5.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data6.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data7.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data8.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data9.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data10.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data11.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data12.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data13.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data14.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data15.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data16.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data17.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data18.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data19.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data20.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data21.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data22.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data23.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data24.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data25.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data26.csv ...\n",
      "File loaded, now adding in Dataframe...\n"
     ]
    }
   ],
   "source": [
    "# spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource_file_upto')\n",
    "\n",
    "#Reading each RAW file - and saving in RAW_DATA_DF\n",
    "raw_data_stores=[]\n",
    "def fetch_and_load_raw_data(f_now,file_index):\n",
    "    print(\"Fetching data from\",f_now,\"...\")\n",
    "    spark.sparkContext.addFile(f_now)\n",
    "    print(\"File loaded, now adding in Dataframe...\")    \n",
    "    return spark.read.option(\"header\", \"true\").csv(SparkFiles.get(\"raw_data{file_index}.csv\".format(file_index=file_index)))\n",
    "\n",
    "print(\"Fetching first raw data file\")\n",
    "\n",
    "f_now = spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource').format(file_index=1)\n",
    "# print(f_now)\n",
    "raw_data_stores.append(fetch_and_load_raw_data(f_now,1))\n",
    "\n",
    "for file_index in range(2, int(spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource_file_upto'))):\n",
    "    f_now = spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource').format(file_index=file_index)  \n",
    "    raw_data_stores.append(fetch_and_load_raw_data(f_now,file_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2415c430-70ee-4344-847d-cda29161e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now working on raw data 1\n",
      "Obtained rows 10819\n",
      "Now working on raw data 2\n",
      "Obtained rows 10020\n",
      "Now working on raw data 3\n",
      "Obtained rows 18231\n",
      "Now working on raw data 4\n",
      "Obtained rows 20488\n",
      "Now working on raw data 5\n",
      "Obtained rows 23423\n",
      "Now working on raw data 6\n",
      "Obtained rows 22770\n",
      "Now working on raw data 7\n",
      "Obtained rows 22808\n",
      "Now working on raw data 8\n",
      "Obtained rows 26897\n",
      "Now working on raw data 9\n",
      "Obtained rows 23112\n",
      "Now working on raw data 10\n",
      "Obtained rows 29045\n",
      "Now working on raw data 11\n",
      "Obtained rows 22334\n",
      "Now working on raw data 12\n",
      "Obtained rows 24252\n",
      "Now working on raw data 13\n",
      "Obtained rows 27583\n",
      "Now working on raw data 14\n",
      "Obtained rows 27346\n",
      "Now working on raw data 15\n",
      "Obtained rows 26625\n",
      "Now working on raw data 16\n",
      "Obtained rows 27286\n",
      "Now working on raw data 17\n",
      "Obtained rows 24636\n",
      "Now working on raw data 18\n",
      "Obtained rows 25384\n",
      "Now working on raw data 19\n",
      "Obtained rows 26310\n",
      "Now working on raw data 20\n",
      "Obtained rows 25496\n",
      "Now working on raw data 21\n",
      "Obtained rows 25791\n",
      "Now working on raw data 22\n",
      "Obtained rows 24549\n",
      "Now working on raw data 23\n",
      "Obtained rows 31273\n",
      "Now working on raw data 24\n",
      "Obtained rows 27733\n",
      "Now working on raw data 25\n",
      "Obtained rows 24925\n"
     ]
    }
   ],
   "source": [
    "# raw_data_stores[0].printSchema()\n",
    "\n",
    "cols_of_interest=['Date Announced','Detected State','Detected District','Gender','Age Bracket','Detected City','Nationality','Current Status','Status Change Date','Num Cases']\n",
    "\n",
    "merged_data =raw_data_stores[0].select(cols_of_interest)\n",
    "for i in range(1,len(raw_data_stores)):\n",
    "    print(\"Now working on raw data\",i)\n",
    "    df = raw_data_stores[i].select(cols_of_interest)\n",
    "    print(\"Obtained rows\",df.count())\n",
    "    merged_data=merged_data.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cffa8c-4be8-4e5f-80d2-ffabbf7f274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date Announced: string (nullable = true)\n",
      " |-- Detected State: string (nullable = true)\n",
      " |-- Detected District: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age Bracket: string (nullable = true)\n",
      " |-- Detected City: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Current Status: string (nullable = true)\n",
      " |-- Status Change Date: string (nullable = true)\n",
      " |-- Num Cases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
    "merged_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254b792d-1161-409c-9d11-12469334471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_1=merged_data.select([to_date(\"Date Announced\",\"dd/MM/yyyy\").alias('Date'),\n",
    "                                  col('Detected State').alias('State'),\n",
    "                                  col('Detected District').alias('District'),\n",
    "                                  'Gender',\n",
    "                                  col('Age Bracket').alias('Age').cast(IntegerType()),\n",
    "                                  col('Detected City').alias('City'),\n",
    "                                  'Nationality',\n",
    "                                  col('Current Status').alias('Status'),\n",
    "                                  to_date('Status Change Date',\"dd/MM/yyyy\").alias('Status_Date'),\n",
    "                                  col('Num Cases').alias('cases').cast(IntegerType())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ceaf02-3132-44b4-bce1-8ea3b314bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------+----+--------------------+-----------+------------+-----------+-----+\n",
      "|      Date|    State|  District|Gender| Age|                City|Nationality|      Status|Status_Date|cases|\n",
      "+----------+---------+----------+------+----+--------------------+-----------+------------+-----------+-----+\n",
      "|2020-01-30|   Kerala|  Thrissur|     F|  20|            Thrissur|      India|   Recovered| 2020-02-14|    1|\n",
      "|2020-02-02|   Kerala| Alappuzha|  null|null|           Alappuzha|      India|   Recovered| 2020-02-14|    1|\n",
      "|2020-02-03|   Kerala| Kasaragod|  null|null|           Kasaragod|      India|   Recovered| 2020-02-14|    1|\n",
      "|2020-03-02|    Delhi|East Delhi|     M|  45|East Delhi (Mayur...|      India|   Recovered| 2020-03-15|    1|\n",
      "|2020-03-02|Telangana| Hyderabad|     M|  24|           Hyderabad|      India|   Recovered| 2020-03-02|    1|\n",
      "|2020-03-03|Rajasthan|  Italians|     M|  69|              Jaipur|      Italy|   Recovered| 2020-03-03|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|Hospitalized| 2020-03-04|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|Hospitalized| 2020-03-04|    1|\n",
      "+----------+---------+----------+------+----+--------------------+-----------+------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08af990f-24a0-41d8-b9b1-7aa6d9b2a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status_Date: date (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ee4df0a-4a42-42b8-a96c-48c8c1bc61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shall we handle data merge after Pivot or before pivoting??\n",
    "import pyspark.sql.functions as fns\n",
    "pivoted_data=merged_data_1.withColumn('Status',fns.lower(col('Status'))).groupBy([\"Date\",\"State\",\"District\"]).pivot(\"Status\").agg({\"Cases\":'sum'})\n",
    "\n",
    "#merged_data_1.coalesce(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4ab1182-39c6-4e7c-b9a0-d91a18022474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- null: long (nullable = true)\n",
      " |-- deceased: long (nullable = true)\n",
      " |-- hospitalized: long (nullable = true)\n",
      " |-- migrated: long (nullable = true)\n",
      " |-- migrated_other: long (nullable = true)\n",
      " |-- recovered: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "root\n",
    " |-- Effective_date: string (nullable = true)\n",
    " |-- State: string (nullable = true)\n",
    " |-- District: string (nullable = true)\n",
    " |-- Confirmed: integer (nullable = true)\n",
    " |-- Recovered: integer (nullable = true)\n",
    " |-- Deceased: integer (nullable = true)\n",
    " |-- Tested: integer (nullable = true)\n",
    "\n",
    "\"\"\"\n",
    "pivoted_data.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8679f051-48b3-4fce-bc00-62192b033a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------------------+----+--------+------------+--------+--------------+---------+\n",
      "|      Date|            State|           District|null|deceased|hospitalized|migrated|migrated_other|recovered|\n",
      "+----------+-----------------+-------------------+----+--------+------------+--------+--------------+---------+\n",
      "|2020-05-01|Jammu and Kashmir|          Baramulla|null|    null|           1|    null|          null|        5|\n",
      "|2020-06-25|    Uttar Pradesh|             Hardoi|null|    null|        null|    null|          null|        5|\n",
      "|2020-08-01|   Andhra Pradesh|            Kurnool|null|       6|        1234|    null|          null|     1217|\n",
      "|2020-10-17|    Uttar Pradesh|          Kaushambi|null|    null|          16|    null|          null|       12|\n",
      "|2020-10-25|      Uttarakhand|           Haridwar|null|    null|          30|    null|             1|       26|\n",
      "|2020-11-15|   Madhya Pradesh|        Hoshangabad|null|       1|          17|    null|          null|        9|\n",
      "|2020-12-14|Jammu and Kashmir|            Rajouri|null|    null|           3|    null|          null|       10|\n",
      "|2020-12-22|    Uttar Pradesh|Gautam Buddha Nagar|null|    null|          46|    null|          null|       53|\n",
      "|2020-12-28|    Uttar Pradesh|       Kanpur Nagar|null|    null|          30|    null|          null|       40|\n",
      "|2021-01-13|           Punjab|            Barnala|null|    null|           4|    null|          null|        1|\n",
      "|2021-02-21|           Punjab|         Tarn Taran|null|       1|           2|    null|          null|        3|\n",
      "|2021-03-14|       Tamil Nadu|         Thiruvarur|null|    null|          10|    null|          null|        4|\n",
      "|2021-03-28|      Maharashtra|           Bhandara|null|       1|         425|    null|             1|      113|\n",
      "|2021-04-27|      Maharashtra|             Mumbai|null|      59|        3999|    null|            43|     7524|\n",
      "|2021-05-02|           Kerala|          Alappuzha|null|    null|        2442|    null|          null|      970|\n",
      "|2020-05-09|       Tamil Nadu|            Madurai|null|    null|        null|    null|          null|       16|\n",
      "|2020-07-21|    Uttar Pradesh|             Meerut|null|       2|          37|    null|          null|       52|\n",
      "|2020-09-16|        Karnataka|   Dakshina Kannada|null|      -1|         466|    null|          null|     null|\n",
      "|2020-09-18|   Madhya Pradesh|             Indore|null|       6|         396|    null|          null|      920|\n",
      "|2021-01-30|     Chhattisgarh|              Sukma|null|    null|           2|    null|          null|     null|\n",
      "+----------+-----------------+-------------------+----+--------+------------+--------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef844123-302f-4fb6-8553-bbe1f4f198ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_df = pivoted_data.select(col('Date').alias('Effective_date'),\n",
    "                                   col('State'),\n",
    "                                   col('District'),\n",
    "                                   col('hospitalized').alias('Confirmed'),\n",
    "                                   col('recovered').alias('Recovered'),\n",
    "                                   col('deceased').alias('Deceased'))\n",
    "\n",
    "# we are removing those lines where none of the data is numeric - this cleans the entire data set as well\n",
    "# we can use filter or where either\n",
    "districts_df = districts_df.filter(col('Confirmed').isNotNull() | col('Recovered').isNotNull() | col('Deceased').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3e7fb6c-a965-4e64-a8a9-5b6d62b7ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Effective_date: date (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Confirmed: long (nullable = true)\n",
      " |-- Recovered: long (nullable = true)\n",
      " |-- Deceased: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09239613-6181-49b6-b873-a3dfa431185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------+------------------+----------------+------------------+\n",
      "|summary|               State|     District|         Confirmed|       Recovered|          Deceased|\n",
      "+-------+--------------------+-------------+------------------+----------------+------------------+\n",
      "|  count|              221253|       218366|            207149|          172788|             56320|\n",
      "|   mean|                null|         null|100.19934443323405|98.3832962937241|       4.013671875|\n",
      "| stddev|                null|         null| 468.4390237370644|446.040692572256|11.878261039782648|\n",
      "|    min|Andaman and Nicob...| Kamrup Rural|            -12822|          -11132|              -275|\n",
      "|    25%|                null|         null|                 5|               5|                 1|\n",
      "|    50%|                null|         null|                17|              17|                 2|\n",
      "|    75%|                null|         null|                61|              61|                 3|\n",
      "|    max|         West Bengal|      vidisha|             28395|           27421|               917|\n",
      "+-------+--------------------+-------------+------------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8590d08-9281-4ca8-baa4-b5d559f9aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------+-------------+--------------+\n",
      "|State                                   |sum(Recovered)|sum(Deceased)|sum(Confirmed)|\n",
      "+----------------------------------------+--------------+-------------+--------------+\n",
      "|Nagaland                                |12357         |115          |14717         |\n",
      "|Karnataka                               |1209892       |16530        |1690862       |\n",
      "|Odisha                                  |416368        |2157         |489640        |\n",
      "|Kerala                                  |1362079       |5565         |1743872       |\n",
      "|Dadra and Nagar Haveli and Daman and Diu|6541          |4            |8345          |\n",
      "|Ladakh                                  |13019         |151          |14560         |\n",
      "|State Unassigned                        |null          |null         |0             |\n",
      "|Tamil Nadu                              |1108436       |14589        |1249285       |\n",
      "|Chhattisgarh                            |653518        |9485         |787478        |\n",
      "|Andhra Pradesh                          |1015913       |8258         |1184026       |\n",
      "|Lakshadweep                             |2065          |6            |3249          |\n",
      "|Madhya Pradesh                          |519722        |5902         |612664        |\n",
      "|Punjab                                  |327892        |9629         |399554        |\n",
      "|null                                    |36            |null         |1             |\n",
      "|Manipur                                 |30140         |424          |32955         |\n",
      "|Goa                                     |74984         |1443         |104398        |\n",
      "|Mizoram                                 |5243          |17           |6794          |\n",
      "|Himachal Pradesh                        |85650         |1646         |110943        |\n",
      "|Puducherry                              |52513         |883          |65117         |\n",
      "|Jammu and Kashmir                       |154310        |2505         |196584        |\n",
      "|Haryana                                 |429763        |4776         |543547        |\n",
      "|Jharkhand                               |194421        |3202         |257345        |\n",
      "|Arunachal Pradesh                       |17500         |59           |19193         |\n",
      "|Gujarat                                 |464084        |7632         |620466        |\n",
      "|Sikkim                                  |6515          |155          |8919          |\n",
      "|Delhi                                   |1143105       |18010        |1253898       |\n",
      "|Chandigarh                              |37271         |518          |45974         |\n",
      "|Rajasthan                               |465684        |4826         |668217        |\n",
      "|Andaman and Nicobar Islands             |5879          |70           |6170          |\n",
      "|Assam                                   |237061        |1429         |267925        |\n",
      "|Meghalaya                               |15957         |190          |18283         |\n",
      "|Maharashtra                             |4105907       |71411        |4822888       |\n",
      "|West Bengal                             |765741        |11727        |898527        |\n",
      "|Telangana                               |389176        |2503         |469720        |\n",
      "|Bihar                                   |410428        |2925         |523840        |\n",
      "|Tripura                                 |33869         |397          |36231         |\n",
      "|Uttar Pradesh                           |1081498       |13769        |1368174       |\n",
      "|Uttarakhand                             |144916        |3142         |211833        |\n",
      "+----------------------------------------+--------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_df.select([\"State\",\"Confirmed\",\"Recovered\",\"Deceased\"]).where(col(\"State\").isNotNull()).groupBy(\"State\").agg({'Confirmed':'sum','Recovered':'sum','Deceased':'sum'}).show(40,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326e921-feff-4861-9cb2-f01cd7a133f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7ebf094-332d-4270-b665-14f61e2e782a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e152ebca9e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistricts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merged_and_clean_raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "districts_df.write.partitionBy(\"State\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .saveAsTable(\"merged_and_clean_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "347d4903-f8f1-4915-a8d4-17183a8876a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+\n",
      "|    a|    b|   c|   d|\n",
      "+-----+-----+----+----+\n",
      "|  121|  242|  35| 555|\n",
      "| 1121| 2432| 353|1343|\n",
      "|12121|24342|3534|1111|\n",
      "+-----+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def duplicate_col_remover(df):\n",
    "    unique_cols = []\n",
    "    releated_cols = []    \n",
    "    for i in range(len(df.columns)):\n",
    "        if df.columns[i] not in unique_cols:\n",
    "            unique_cols.append(df.columns[i])\n",
    "        else:\n",
    "            releated_cols.append(i)\n",
    "    col_set=[]\n",
    "    for i in range(len(df.columns)):\n",
    "        col_set.append(str(i))\n",
    "\n",
    "    df = df.toDF(*col_set)\n",
    "    for dupcol in releated_cols:\n",
    "        df = df.drop(str(dupcol))\n",
    "    return df.toDF(*unique_cols)\n",
    "\n",
    "d1 = spark.createDataFrame([{\"a\":12121,\"b\":24342,\"c\":3534},{\"a\":121,\"b\":242,\"c\":35},{\"a\":1121,\"b\":2432,\"c\":353},{\"a\":121,\"b\":242,\"c\":34}])\n",
    "d2 =  spark.createDataFrame([{\"d\":1111,\"b\":24342,\"c\":3534},{\"d\":555,\"b\":242,\"c\":35},{\"d\":1343,\"b\":2432,\"c\":353},{\"d\":434,\"b\":43,\"c\":34}])\n",
    "\n",
    "joint_housing_table  = d1.join(d2, (d1.b == d2.b) & (d1.c == d2.c),\"inner\")\n",
    "\n",
    "duplicate_col_remover(joint_housing_table).show()\n",
    "\n",
    "# joint_housing_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07869fd6-8ca8-4951-bec0-cf8dcc1de275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date Announced: string (nullable = true)\n",
      " |-- Detected State: string (nullable = true)\n",
      " |-- Detected District: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age Bracket: string (nullable = true)\n",
      " |-- Detected City: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Current Status: string (nullable = true)\n",
      " |-- Status Change Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca164538-98d0-45e3-a9ab-1a4e5affe256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|      Detected State|count(Detected District)|\n",
      "+--------------------+------------------------+\n",
      "|               63855|                       0|\n",
      "|            Nagaland|                    2063|\n",
      "|           Karnataka|                  123855|\n",
      "|              Odisha|                   20261|\n",
      "|              Kerala|                   17070|\n",
      "|              Ladakh|                    1202|\n",
      "|Dadra and Nagar H...|                    1417|\n",
      "|    State Unassigned|                      63|\n",
      "|               63863|                       0|\n",
      "|          Tamil Nadu|                   52355|\n",
      "|               63851|                       0|\n",
      "|               63817|                       0|\n",
      "|        Chhattisgarh|                   19042|\n",
      "|      Andhra Pradesh|                   12952|\n",
      "|         Lakshadweep|                     202|\n",
      "|      Madhya Pradesh|                   35712|\n",
      "|              Punjab|                   16988|\n",
      "|        Using RT-PCR|                       0|\n",
      "|               63852|                       0|\n",
      "|               63823|                       0|\n",
      "+--------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data.select(['Detected State','Detected District']).groupBy('Detected State').agg({'Detected District':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dcc7a62-7f3d-4c76-9d4a-01362b4dd44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded, now adding in Dataframe...\n"
     ]
    }
   ],
   "source": [
    "f_now='https://api.covid19india.org/csv/latest/case_time_series.csv'\n",
    "spark.sparkContext.addFile(f_now)\n",
    "print(\"File loaded, now adding in Dataframe...\")    \n",
    "case_ts_data = spark.read.option(\"header\", \"true\").csv(SparkFiles.get(\"case_time_series.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ef5d000-eacd-45ff-b61c-c21452e5e701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "|            Date|  Date_YMD|Daily Confirmed|Total Confirmed|Daily Recovered|Total Recovered|Daily Deceased|Total Deceased|\n",
      "+----------------+----------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "| 30 January 2020|2020-01-30|              1|              1|              0|              0|             0|             0|\n",
      "| 31 January 2020|2020-01-31|              0|              1|              0|              0|             0|             0|\n",
      "| 1 February 2020|2020-02-01|              0|              1|              0|              0|             0|             0|\n",
      "| 2 February 2020|2020-02-02|              1|              2|              0|              0|             0|             0|\n",
      "| 3 February 2020|2020-02-03|              1|              3|              0|              0|             0|             0|\n",
      "| 4 February 2020|2020-02-04|              0|              3|              0|              0|             0|             0|\n",
      "| 5 February 2020|2020-02-05|              0|              3|              0|              0|             0|             0|\n",
      "| 6 February 2020|2020-02-06|              0|              3|              0|              0|             0|             0|\n",
      "| 7 February 2020|2020-02-07|              0|              3|              0|              0|             0|             0|\n",
      "| 8 February 2020|2020-02-08|              0|              3|              0|              0|             0|             0|\n",
      "| 9 February 2020|2020-02-09|              0|              3|              0|              0|             0|             0|\n",
      "|10 February 2020|2020-02-10|              0|              3|              0|              0|             0|             0|\n",
      "|11 February 2020|2020-02-11|              0|              3|              0|              0|             0|             0|\n",
      "|12 February 2020|2020-02-12|              0|              3|              0|              0|             0|             0|\n",
      "|13 February 2020|2020-02-13|              0|              3|              1|              1|             0|             0|\n",
      "|14 February 2020|2020-02-14|              0|              3|              0|              1|             0|             0|\n",
      "|15 February 2020|2020-02-15|              0|              3|              0|              1|             0|             0|\n",
      "|16 February 2020|2020-02-16|              0|              3|              1|              2|             0|             0|\n",
      "|17 February 2020|2020-02-17|              0|              3|              0|              2|             0|             0|\n",
      "|18 February 2020|2020-02-18|              0|              3|              0|              2|             0|             0|\n",
      "+----------------+----------+---------------+---------------+---------------+---------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_ts_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5208f759-67f7-44c7-a4db-86338f9c5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_data= (spark.read   \n",
    "                .option(\"sep\", \",\").option(\"header\",True)\n",
    "                .csv('./Data/districts.csv'))\n",
    "\n",
    "\n",
    "# districts_data = sc.textFile('./Data/districts.csv')\n",
    "# # districts_data = sc.parallelize(districts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8361f6-bb64-45a2-82e2-84d2e188f390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------+-------------+---------+---------+--------+-----+------+\n",
      "|Date      |State                      |District     |Confirmed|Recovered|Deceased|Other|Tested|\n",
      "+----------+---------------------------+-------------+---------+---------+--------+-----+------+\n",
      "|2020-04-26|Andaman and Nicobar Islands|Unknown      |33       |11       |0       |0    |2679  |\n",
      "|2020-04-26|Andhra Pradesh             |Anantapur    |53       |14       |4       |0    |null  |\n",
      "|2020-04-26|Andhra Pradesh             |Chittoor     |73       |13       |0       |0    |null  |\n",
      "|2020-04-26|Andhra Pradesh             |East Godavari|39       |12       |0       |0    |null  |\n",
      "|2020-04-26|Andhra Pradesh             |Guntur       |214      |29       |8       |0    |null  |\n",
      "+----------+---------------------------+-------------+---------+---------+--------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_data.show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dc1b634-b5a7-44f6-8af1-2063aa1a2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Confirmed: string (nullable = true)\n",
      " |-- Recovered: string (nullable = true)\n",
      " |-- Deceased: string (nullable = true)\n",
      " |-- Other: string (nullable = true)\n",
      " |-- Tested: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_data.printSchema()\n",
    "# districts_data = districts_data.map(lambda l:l.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66c81c22-3ce2-441b-bf7f-4488a2df861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+----------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|      Date|               State|  District|         Confirmed|         Recovered|          Deceased|            Other|            Tested|\n",
      "+-------+----------+--------------------+----------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|    229783|              229783|    229783|            229783|            229783|            229783|           229783|            167134|\n",
      "|   mean|      null|                null|      null|10142.690882267183| 9305.670776341156|149.81697079418407|3.199187929481293|159733.46884535762|\n",
      "| stddev|      null|                null|      null|33782.449718313896|31427.514838583946| 629.6567303101169|41.49328159948047| 559857.5480608118|\n",
      "|    min|2020-04-26|Andaman and Nicob...|Agar Malwa|                -1|                -1|                -1|               -1|                 1|\n",
      "|    max|2021-04-19|         West Bengal| Zunheboto|              9999|              9999|               999|              995|             99998|\n",
      "+-------+----------+--------------------+----------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e3103c-0819-49f6-afd9-4e490ad3b90a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'districts_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d687bbe9434a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistricts_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'District'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'districts_data' is not defined"
     ]
    }
   ],
   "source": [
    "districts_data.unique('District')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "870fdf54-9dda-4ac5-ae1c-a87ed16e0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b160e7a5-5547-4138-b2fb-469af71c4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_csv_req=requests.get(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource'))\n",
    "# stock_csv_req.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45a6765c-833e-468e-b9b9-05b339b2e7c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o256.load.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1(SessionState.scala:102)\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1$adapted(SessionState.scala:102)\n\tat scala.collection.immutable.Map$Map2.foreach(Map.scala:159)\n\tat org.apache.spark.sql.internal.SessionState.newHadoopConfWithOptions(SessionState.scala:102)\n\tat org.apache.spark.sql.execution.datasources.DataSource.newHadoopConfiguration(DataSource.scala:114)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:375)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a896e1f0f71f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# spark.sparkContext.addFile(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_stock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_csv_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_stock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o256.load.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1(SessionState.scala:102)\n\tat org.apache.spark.sql.internal.SessionState.$anonfun$newHadoopConfWithOptions$1$adapted(SessionState.scala:102)\n\tat scala.collection.immutable.Map$Map2.foreach(Map.scala:159)\n\tat org.apache.spark.sql.internal.SessionState.newHadoopConfWithOptions(SessionState.scala:102)\n\tat org.apache.spark.sql.execution.datasources.DataSource.newHadoopConfiguration(DataSource.scala:114)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:375)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n"
     ]
    }
   ],
   "source": [
    "# print(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource'))\n",
    "#Load df_stock\n",
    "# spark.sparkContext.addFile(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource'))\n",
    "\n",
    "df_stock = spark.read.option(\"header\", \"true\").format(\"csv\").csv(io.StringIO(stock_csv_req.text))\n",
    "\n",
    "df_stock.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4022a699-dd1a-4f76-8ed1-ab78b2971102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /private/var/folders/3y/b7xzrwss1wg2drn7pkcst00m0000gn/T/spark-b44425e1-fa03-4e51-bf95-ff5fe3616da4/userFiles-1f24d8e8-41d9-47b0-adcd-0040e75b8faa/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /private/var/folders/3y/b7xzrwss1wg2drn7pkcst00m0000gn/T/spark-b44425e1-fa03-4e51-bf95-ff5fe3616da4/userFiles-1f24d8e8-41d9-47b0-adcd-0040e75b8faa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef406c-dc10-499a-9ffb-f47d88fb12bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
