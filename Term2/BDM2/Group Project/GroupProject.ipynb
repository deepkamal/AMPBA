{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b91c2c4-09ff-4b2c-8adc-152815336ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from pyspark) (0.10.9)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e0b866-5d8c-4390-9de9-fd269aaaf5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fb68f5-c91d-42b5-9a0d-5409c832c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SparkFiles\n",
    "from pyspark.sql import *\n",
    "\n",
    "\n",
    "config_prefix = \"ampba.batch15.bdm2.group_assignment\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"BDM2-GroupAssignment\") \\\n",
    ".config(config_prefix+\".raw_covid_datasource\", \"https://api.covid19india.org/csv/latest/raw_data{file_index}.csv\") \\\n",
    ".config(config_prefix+\".raw_covid_datasource_file_upto\", 27) \\\n",
    ".config(config_prefix+\".covid_datasource\", \"https://api.covid19india.org/csv/latest/districts.csv\") \\\n",
    ".config(config_prefix+\".stock_datasource\", \"https://docs.google.com/spreadsheets/d/1sNXNbIrOSU6jdJwFHOY6FGGM8PsJrNdkWf_70WOPtjc/export?format=csv\") \\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9390da1b-884f-4d9f-b220-cd37ff8fd223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BDM2-GroupAssignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a6aaa90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3713586a-db5b-4f59-860a-4f55b2a3bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching first raw data file\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data1.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data2.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data3.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data4.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data5.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data6.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data7.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data8.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data9.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data10.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data11.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data12.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data13.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data14.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data15.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data16.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data17.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data18.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data19.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data20.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data21.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data22.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data23.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data24.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data25.csv ...\n",
      "File loaded, now adding in Dataframe...\n",
      "Fetching data from https://api.covid19india.org/csv/latest/raw_data26.csv ...\n",
      "File loaded, now adding in Dataframe...\n"
     ]
    }
   ],
   "source": [
    "# spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource_file_upto')\n",
    "\n",
    "#Reading each RAW file - and saving in RAW_DATA_DF\n",
    "raw_data_stores=[]\n",
    "def fetch_and_load_raw_data(f_now,file_index):\n",
    "    print(\"Fetching data from\",f_now,\"...\")\n",
    "    spark.sparkContext.addFile(f_now)\n",
    "    print(\"File loaded, now adding in Dataframe...\")    \n",
    "    return spark.read.option(\"header\", \"true\").csv(SparkFiles.get(\"raw_data{file_index}.csv\".format(file_index=file_index)))\n",
    "\n",
    "print(\"Fetching first raw data file\")\n",
    "\n",
    "f_now = spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource').format(file_index=1)\n",
    "# print(f_now)\n",
    "raw_data_stores.append(fetch_and_load_raw_data(f_now,1))\n",
    "\n",
    "for file_index in range(2, int(spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource_file_upto'))):\n",
    "    f_now = spark.sparkContext.getConf().get(config_prefix+'.raw_covid_datasource').format(file_index=file_index)  \n",
    "    raw_data_stores.append(fetch_and_load_raw_data(f_now,file_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2415c430-70ee-4344-847d-cda29161e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now working on raw data 1\n",
      "Obtained rows 10819\n",
      "Now working on raw data 2\n",
      "Obtained rows 10020\n",
      "Now working on raw data 3\n",
      "Obtained rows 18231\n",
      "Now working on raw data 4\n",
      "Obtained rows 20488\n",
      "Now working on raw data 5\n",
      "Obtained rows 23423\n",
      "Now working on raw data 6\n",
      "Obtained rows 22770\n",
      "Now working on raw data 7\n",
      "Obtained rows 22808\n",
      "Now working on raw data 8\n",
      "Obtained rows 26897\n",
      "Now working on raw data 9\n",
      "Obtained rows 23112\n",
      "Now working on raw data 10\n",
      "Obtained rows 29045\n",
      "Now working on raw data 11\n",
      "Obtained rows 22334\n",
      "Now working on raw data 12\n",
      "Obtained rows 24252\n",
      "Now working on raw data 13\n",
      "Obtained rows 27583\n",
      "Now working on raw data 14\n",
      "Obtained rows 27346\n",
      "Now working on raw data 15\n",
      "Obtained rows 26625\n",
      "Now working on raw data 16\n",
      "Obtained rows 27286\n",
      "Now working on raw data 17\n",
      "Obtained rows 24636\n",
      "Now working on raw data 18\n",
      "Obtained rows 25384\n",
      "Now working on raw data 19\n",
      "Obtained rows 26310\n",
      "Now working on raw data 20\n",
      "Obtained rows 25496\n",
      "Now working on raw data 21\n",
      "Obtained rows 25791\n",
      "Now working on raw data 22\n",
      "Obtained rows 24549\n",
      "Now working on raw data 23\n",
      "Obtained rows 31273\n",
      "Now working on raw data 24\n",
      "Obtained rows 27733\n",
      "Now working on raw data 25\n",
      "Obtained rows 24063\n"
     ]
    }
   ],
   "source": [
    "# raw_data_stores[0].printSchema()\n",
    "\n",
    "cols_of_interest=['Date Announced','Detected State','Detected District','Gender','Age Bracket','Detected City','Nationality','Current Status','Status Change Date','Num Cases']\n",
    "\n",
    "merged_data =raw_data_stores[0].select(cols_of_interest)\n",
    "for i in range(1,len(raw_data_stores)):\n",
    "    print(\"Now working on raw data\",i)\n",
    "    df = raw_data_stores[i].select(cols_of_interest)\n",
    "    print(\"Obtained rows\",df.count())\n",
    "    merged_data=merged_data.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cffa8c-4be8-4e5f-80d2-ffabbf7f274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date Announced: string (nullable = true)\n",
      " |-- Detected State: string (nullable = true)\n",
      " |-- Detected District: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age Bracket: string (nullable = true)\n",
      " |-- Detected City: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Current Status: string (nullable = true)\n",
      " |-- Status Change Date: string (nullable = true)\n",
      " |-- Num Cases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
    "merged_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254b792d-1161-409c-9d11-12469334471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_1=merged_data.select([to_date(\"Date Announced\",\"dd/MM/yyyy\").alias('Date'),\n",
    "                                  col('Detected State').alias('State'),\n",
    "                                  col('Detected District').alias('District'),\n",
    "                                  'Gender',\n",
    "                                  col('Age Bracket').alias('Age').cast(IntegerType()),\n",
    "                                  col('Detected City').alias('City'),\n",
    "                                  'Nationality',\n",
    "                                  col('Current Status').alias('Status'),\n",
    "                                  to_date('Status Change Date',\"dd/MM/yyyy\").alias('Status_Date'),\n",
    "                                  col('Num Cases').alias('cases').cast(IntegerType())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ceaf02-3132-44b4-bce1-8ea3b314bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------+----+--------------------+-----------+------------+-----------+-----+\n",
      "|      Date|    State|  District|Gender| Age|                City|Nationality|      Status|Status_Date|cases|\n",
      "+----------+---------+----------+------+----+--------------------+-----------+------------+-----------+-----+\n",
      "|2020-01-30|   Kerala|  Thrissur|     F|  20|            Thrissur|      India|   Recovered| 2020-02-14|    1|\n",
      "|2020-02-02|   Kerala| Alappuzha|  null|null|           Alappuzha|      India|   Recovered| 2020-02-14|    1|\n",
      "|2020-02-03|   Kerala| Kasaragod|  null|null|           Kasaragod|      India|   Recovered| 2020-02-14|    1|\n",
      "|2020-03-02|    Delhi|East Delhi|     M|  45|East Delhi (Mayur...|      India|   Recovered| 2020-03-15|    1|\n",
      "|2020-03-02|Telangana| Hyderabad|     M|  24|           Hyderabad|      India|   Recovered| 2020-03-02|    1|\n",
      "|2020-03-03|Rajasthan|  Italians|     M|  69|              Jaipur|      Italy|   Recovered| 2020-03-03|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|   Recovered| 2020-03-29|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|Hospitalized| 2020-03-04|    1|\n",
      "|2020-03-04|  Haryana|  Italians|  null|  55|            Gurugram|      Italy|Hospitalized| 2020-03-04|    1|\n",
      "+----------+---------+----------+------+----+--------------------+-----------+------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08af990f-24a0-41d8-b9b1-7aa6d9b2a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status_Date: date (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee4df0a-4a42-42b8-a96c-48c8c1bc61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shall we handle data merge after Pivot or before pivoting??\n",
    "import pyspark.sql.functions as fns\n",
    "pivoted_data=merged_data_1.withColumn('Status',fns.lower(col('Status'))).groupBy([\"Date\",\"State\",\"District\"]).pivot(\"Status\").agg({\"Cases\":'sum'})\n",
    "\n",
    "#merged_data_1.coalesce(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ab1182-39c6-4e7c-b9a0-d91a18022474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- null: long (nullable = true)\n",
      " |-- deceased: long (nullable = true)\n",
      " |-- hospitalized: long (nullable = true)\n",
      " |-- migrated: long (nullable = true)\n",
      " |-- migrated_other: long (nullable = true)\n",
      " |-- recovered: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "root\n",
    " |-- Effective_date: string (nullable = true)\n",
    " |-- State: string (nullable = true)\n",
    " |-- District: string (nullable = true)\n",
    " |-- Confirmed: integer (nullable = true)\n",
    " |-- Recovered: integer (nullable = true)\n",
    " |-- Deceased: integer (nullable = true)\n",
    " |-- Tested: integer (nullable = true)\n",
    "\n",
    "\"\"\"\n",
    "pivoted_data.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8679f051-48b3-4fce-bc00-62192b033a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------------------+----+--------+------------+--------+--------------+---------+\n",
      "|      Date|            State|           District|null|deceased|hospitalized|migrated|migrated_other|recovered|\n",
      "+----------+-----------------+-------------------+----+--------+------------+--------+--------------+---------+\n",
      "|2020-05-01|Jammu and Kashmir|          Baramulla|null|    null|           1|    null|          null|        5|\n",
      "|2020-06-25|    Uttar Pradesh|             Hardoi|null|    null|        null|    null|          null|        5|\n",
      "|2020-08-01|   Andhra Pradesh|            Kurnool|null|       6|        1234|    null|          null|     1217|\n",
      "|2020-10-17|    Uttar Pradesh|          Kaushambi|null|    null|          16|    null|          null|       12|\n",
      "|2020-10-25|      Uttarakhand|           Haridwar|null|    null|          30|    null|             1|       26|\n",
      "|2020-11-15|   Madhya Pradesh|        Hoshangabad|null|       1|          17|    null|          null|        9|\n",
      "|2020-12-14|Jammu and Kashmir|            Rajouri|null|    null|           3|    null|          null|       10|\n",
      "|2020-12-22|    Uttar Pradesh|Gautam Buddha Nagar|null|    null|          46|    null|          null|       53|\n",
      "|2020-12-28|    Uttar Pradesh|       Kanpur Nagar|null|    null|          30|    null|          null|       40|\n",
      "|2021-01-13|           Punjab|            Barnala|null|    null|           4|    null|          null|        1|\n",
      "|2021-02-21|           Punjab|         Tarn Taran|null|       1|           2|    null|          null|        3|\n",
      "|2021-03-14|       Tamil Nadu|         Thiruvarur|null|    null|          10|    null|          null|        4|\n",
      "|2021-03-28|      Maharashtra|           Bhandara|null|       1|         425|    null|             1|      113|\n",
      "|2021-04-27|      Maharashtra|             Mumbai|null|      59|        3999|    null|            43|     7524|\n",
      "|2021-05-02|           Kerala|          Alappuzha|null|    null|        2442|    null|          null|      970|\n",
      "|2020-05-09|       Tamil Nadu|            Madurai|null|    null|        null|    null|          null|       16|\n",
      "|2020-07-21|    Uttar Pradesh|             Meerut|null|       2|          37|    null|          null|       52|\n",
      "|2020-09-16|        Karnataka|   Dakshina Kannada|null|      -1|         466|    null|          null|     null|\n",
      "|2020-09-18|   Madhya Pradesh|             Indore|null|       6|         396|    null|          null|      920|\n",
      "|2021-01-30|     Chhattisgarh|              Sukma|null|    null|           2|    null|          null|     null|\n",
      "+----------+-----------------+-------------------+----+--------+------------+--------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef844123-302f-4fb6-8553-bbe1f4f198ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_df = pivoted_data.select(col('Date').alias('Effective_date'),\n",
    "                                   col('State'),\n",
    "                                   col('District'),\n",
    "                                   col('hospitalized').alias('Confirmed'),\n",
    "                                   col('recovered').alias('Recovered'),\n",
    "                                   col('deceased').alias('Deceased'))\n",
    "\n",
    "# we are removing those lines where none of the data is numeric - this cleans the entire data set as well\n",
    "# we can use filter or where either\n",
    "districts_df = districts_df.filter(col('Confirmed').isNotNull() | col('Recovered').isNotNull() | col('Deceased').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e7fb6c-a965-4e64-a8a9-5b6d62b7ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Effective_date: date (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Confirmed: long (nullable = true)\n",
      " |-- Recovered: long (nullable = true)\n",
      " |-- Deceased: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09239613-6181-49b6-b873-a3dfa431185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------+-----------------+------------------+-----------------+\n",
      "|summary|               State|     District|        Confirmed|         Recovered|         Deceased|\n",
      "+-------+--------------------+-------------+-----------------+------------------+-----------------+\n",
      "|  count|              221103|       218220|           207002|            172683|            56275|\n",
      "|   mean|                null|         null|99.79734012231766| 98.05392540087907|4.004193691692581|\n",
      "| stddev|                null|         null|465.4096633602333|442.97970033075364|11.80311483244228|\n",
      "|    min|Andaman and Nicob...| Kamrup Rural|           -12822|            -11132|             -275|\n",
      "|    25%|                null|         null|                5|                 5|                1|\n",
      "|    50%|                null|         null|               16|                17|                2|\n",
      "|    75%|                null|         null|               61|                61|                3|\n",
      "|    max|         West Bengal|      vidisha|            28395|             27421|              917|\n",
      "+-------+--------------------+-------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8590d08-9281-4ca8-baa4-b5d559f9aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------+-------------+--------------+\n",
      "|State                                   |sum(Recovered)|sum(Deceased)|sum(Confirmed)|\n",
      "+----------------------------------------+--------------+-------------+--------------+\n",
      "|Nagaland                                |12357         |115          |14717         |\n",
      "|Karnataka                               |1209892       |16530        |1690862       |\n",
      "|Odisha                                  |410192        |2141         |479751        |\n",
      "|Kerala                                  |1338973       |5507         |1701919       |\n",
      "|Dadra and Nagar Haveli and Daman and Diu|6541          |4            |8345          |\n",
      "|Ladakh                                  |13019         |151          |14560         |\n",
      "|State Unassigned                        |null          |null         |0             |\n",
      "|Tamil Nadu                              |1108436       |14589        |1249285       |\n",
      "|Chhattisgarh                            |653518        |9485         |787478        |\n",
      "|Andhra Pradesh                          |1015913       |8258         |1184026       |\n",
      "|Lakshadweep                             |2065          |6            |3249          |\n",
      "|Madhya Pradesh                          |519722        |5902         |612664        |\n",
      "|Punjab                                  |327892        |9629         |399554        |\n",
      "|Manipur                                 |30140         |424          |32955         |\n",
      "|Goa                                     |72792         |1372         |100902        |\n",
      "|Mizoram                                 |5168          |17           |6556          |\n",
      "|Himachal Pradesh                        |85650         |1646         |110943        |\n",
      "|Puducherry                              |51580         |865          |63298         |\n",
      "|Jammu and Kashmir                       |151972        |2453         |191868        |\n",
      "|Haryana                                 |429763        |4776         |543547        |\n",
      "|Jharkhand                               |194421        |3202         |257345        |\n",
      "|Arunachal Pradesh                       |17500         |59           |19193         |\n",
      "|Gujarat                                 |464084        |7632         |620466        |\n",
      "|Sikkim                                  |6426          |151          |8698          |\n",
      "|Delhi                                   |1123896       |17699        |1232938       |\n",
      "|Chandigarh                              |37271         |518          |45974         |\n",
      "|Rajasthan                               |465684        |4826         |668217        |\n",
      "|Andaman and Nicobar Islands             |5879          |70           |6170          |\n",
      "|Assam                                   |237061        |1429         |267925        |\n",
      "|Meghalaya                               |15810         |184          |18014         |\n",
      "|Maharashtra                             |4105907       |71411        |4822888       |\n",
      "|West Bengal                             |765741        |11727        |898527        |\n",
      "|Telangana                               |381050        |2452         |463359        |\n",
      "|Bihar                                   |410428        |2925         |523840        |\n",
      "|Tripura                                 |33810         |397          |35991         |\n",
      "|Uttar Pradesh                           |1081498       |13769        |1368174       |\n",
      "|Uttarakhand                             |140159        |3015         |204050        |\n",
      "+----------------------------------------+--------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "districts_df.select([\"State\",\"Confirmed\",\"Recovered\",\"Deceased\"]).where(col(\"State\").isNotNull()).groupBy(\"State\").agg({'Confirmed':'sum','Recovered':'sum','Deceased':'sum'}).show(40,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326e921-feff-4861-9cb2-f01cd7a133f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ebf094-332d-4270-b665-14f61e2e782a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Can not create the managed table('`merged_and_clean_raw`'). The associated location('file:/Users/dks/AMPBA/Term2/BDM2/Group%20Project/spark-warehouse/merged_and_clean_raw') already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e152ebca9e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistricts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merged_and_clean_raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Can not create the managed table('`merged_and_clean_raw`'). The associated location('file:/Users/dks/AMPBA/Term2/BDM2/Group%20Project/spark-warehouse/merged_and_clean_raw') already exists."
     ]
    }
   ],
   "source": [
    "districts_df.write.partitionBy(\"State\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .saveAsTable(\"merged_and_clean_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d4903-f8f1-4915-a8d4-17183a8876a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_col_remover(df):\n",
    "    unique_cols = []\n",
    "    releated_cols = []    \n",
    "    for i in range(len(df.columns)):\n",
    "        if df.columns[i] not in unique_cols:\n",
    "            unique_cols.append(df.columns[i])\n",
    "        else:\n",
    "            releated_cols.append(i)\n",
    "    col_set=[]\n",
    "    for i in range(len(df.columns)):\n",
    "        col_set.append(str(i))\n",
    "\n",
    "    df = df.toDF(*col_set)\n",
    "    for dupcol in releated_cols:\n",
    "        df = df.drop(str(dupcol))\n",
    "    return df.toDF(*unique_cols)\n",
    "\n",
    "d1 = spark.createDataFrame([{\"a\":12121,\"b\":24342,\"c\":3534},{\"a\":121,\"b\":242,\"c\":35},{\"a\":1121,\"b\":2432,\"c\":353},{\"a\":121,\"b\":242,\"c\":34}])\n",
    "d2 =  spark.createDataFrame([{\"d\":1111,\"b\":24342,\"c\":3534},{\"d\":555,\"b\":242,\"c\":35},{\"d\":1343,\"b\":2432,\"c\":353},{\"d\":434,\"b\":43,\"c\":34}])\n",
    "\n",
    "joint_housing_table  = d1.join(d2, (d1.b == d2.b) & (d1.c == d2.c),\"inner\")\n",
    "\n",
    "duplicate_col_remover(joint_housing_table).show()\n",
    "\n",
    "# joint_housing_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07869fd6-8ca8-4951-bec0-cf8dcc1de275",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca164538-98d0-45e3-a9ab-1a4e5affe256",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.select(['Detected State','Detected District']).groupBy('Detected State').agg({'Detected District':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc7a62-7f3d-4c76-9d4a-01362b4dd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_now='https://api.covid19india.org/csv/latest/case_time_series.csv'\n",
    "spark.sparkContext.addFile(f_now)\n",
    "print(\"File loaded, now adding in Dataframe...\")    \n",
    "case_ts_data = spark.read.option(\"header\", \"true\").csv(SparkFiles.get(\"case_time_series.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5d000-eacd-45ff-b61c-c21452e5e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ts_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208f759-67f7-44c7-a4db-86338f9c5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_data= (spark.read   \n",
    "                .option(\"sep\", \",\").option(\"header\",True)\n",
    "                .csv('./Data/districts.csv'))\n",
    "\n",
    "\n",
    "# districts_data = sc.textFile('./Data/districts.csv')\n",
    "# # districts_data = sc.parallelize(districts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8361f6-bb64-45a2-82e2-84d2e188f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_data.show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1b634-b5a7-44f6-8af1-2063aa1a2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_data.printSchema()\n",
    "# districts_data = districts_data.map(lambda l:l.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c81c22-3ce2-441b-bf7f-4488a2df861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3103c-0819-49f6-afd9-4e490ad3b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_data.unique('District')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160e7a5-5547-4138-b2fb-469af71c4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_csv_req=requests.get(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource'))\n",
    "# stock_csv_req.text\n",
    "import pandas as pd\n",
    "\n",
    "# df_stock=spark.createDataFrame(pd.read_csv(io.StringIO(stock_csv_req.text)))\n",
    "df_stock=spark.createDataFrame(pd.read_csv(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource')))\n",
    "\n",
    "df_stock.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6765c-833e-468e-b9b9-05b339b2e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource'))\n",
    "#Load df_stock\n",
    "# spark.sparkContext.addFile(spark.sparkContext.getConf().get(config_prefix+'.stock_datasource'))\n",
    "\n",
    "df_stock = spark.read.option(\"header\", \"true\").format(\"csv\").csv()\n",
    "\n",
    "df_stock.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022a699-dd1a-4f76-8ed1-ab78b2971102",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /private/var/folders/3y/b7xzrwss1wg2drn7pkcst00m0000gn/T/spark-b44425e1-fa03-4e51-bf95-ff5fe3616da4/userFiles-1f24d8e8-41d9-47b0-adcd-0040e75b8faa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef406c-dc10-499a-9ffb-f47d88fb12bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
